---
title: 'Project 2: Data Mining, Classification, Prediction'
author: "SDS322E"
date: ''
output:
  html_document:
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: yes
  pdf_document:
    toc: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, fig.align = "center", warning = F, message = F,
tidy=TRUE, tidy.opts=list(width.cutoff=60), R.options=list(max.print=100))

class_diag <- function(score, truth, positive, cutoff=.5){

  pred <- factor(score>cutoff,levels=c("TRUE","FALSE"))
  truth <- factor(truth==positive, levels=c("TRUE","FALSE"))

  tab<-table(truth, pred)
  acc=sum(diag(tab))/sum(tab)
  sens=tab[1,1]/rowSums(tab)[1]
  spec=tab[2,2]/rowSums(tab)[2]
  ppv=tab[1,1]/colSums(tab)[1]

#CALCULATE F1
  f1=2*(sens*ppv)/(sens+ppv)
  
#CALCULATE EXACT AUC
  truth<-as.numeric(truth=="TRUE")
  ord<-order(score, decreasing=TRUE)
  score <- score[ord]; truth <- truth[ord]
  TPR=cumsum(truth)/max(1,sum(truth))
  FPR=cumsum(!truth)/max(1,sum(!truth))
  dup<-c(score[-1]>=score[-length(score)], FALSE)
  TPR<-c(0,TPR[!dup],1); FPR<-c(0,FPR[!dup],1)
  n <- length(TPR)
  auc<- sum( ((TPR[-1]+TPR[-n])/2) * (FPR[-1]-FPR[-n]) )
  round(data.frame(acc,sens,spec,ppv,f1,ba=(sens+spec)/2,auc, row.names = "Metrics"),4)
}
```

# Mining, Classification, Prediction

## Sung Joon (SJ) Roh, Sr52225

### Introduction 

For this project, I am excited to work with the airline safety dataset in the fivethirtyeight package. This dataset tackles the serious question: Should Travelers Avoid Flying Airlines That Have Had Crashes in the Past? This dataset includes the variables: airline, a binary variable for if regional subsidiaries are included are not for the specific airline, the available seat kilometers flown every week (number of seats available x number of kilometers flown), the total number of incidents for the years 1985-1999 & 2000-2014, the total number of fatal accidents for the years 1985-1999 & 2000-2014, and the total number of fatalities for the years 1985-1999 & 2000-2014, totaling 9 variables across the dataset. I decided to drop the columns for the years 1985-1999 to make my data more current, but also to help with analysis. 

I found this data by downloading fivethirtyeight, loading the package, and pulling up datasets from the list to see what interested me the most while fitting the requirements for this project. Out of the 6 variables, there is one categorical explanatory variable (airlines), one binary variable (incl_reg_subsidiaries), and the rest are numeric explanatory variables. In total, there are 56 observations, for 56 different airlines. So, there is one observation per group for my categorical variable, and one observation per group for my binary variable. This dataset popped out to me and is interesting because I find that it addresses an important topic, and one that anyone would find to be valuable and meaningful information. For my first project, I selected a topic and dataset that only would interest a select group of people (basketball fans), so I really wanted to make sure that I chose a topic that is more broad and applies to almost everyone. We have all flown before, and this is a fear that will always be there even if a level of comfort has been achieved, so I wanted to look into this dataset more and address the question that really caught my attention: Should travelers avoid flying airlines that have had crashed in the past? 

```{R}
library(tidyverse)
library(fivethirtyeight)
data()

airline_safety1 <- airline_safety %>% select(-incidents_85_99, -fatal_accidents_85_99, -fatalities_85_99)
airline_safety1

library(tidyverse)

# any other code here
```

### Cluster Analysis

```{R}
library(cluster)
# clustering code here
# Partioning around Medoids (PAM) is a robust alternative to k-means
# - better to use this one in practice, # using actual data points

pam_dat<-airline_safety1%>%dplyr::select(avail_seat_km_per_week, incidents_00_14, fatal_accidents_00_14, fatalities_00_14)
sil_width<-vector()
for(i in 2:10){  
  pam_fit <- pam(pam_dat, k = i)  
  sil_width[i] <- pam_fit$silinfo$avg.width  
}
ggplot()+geom_line(aes(x=1:10,y=sil_width))+scale_x_continuous(name="k",breaks=1:10)

set.seed(322)
pam1 <- pam_dat %>% pam(k=3)
pam1
clust_dat <- pam_dat %>% mutate(cluster=as.factor(pam1$clustering))

## gg-plot
pamclust <- clust_dat %>% mutate(cluster=as.factor(pam1$clustering))
pamclust %>% ggplot(aes(avail_seat_km_per_week, incidents_00_14, color = cluster)) + geom_point()

pam1$silinfo$avg.width # Average Silhouette Width # Strong structure: 0.71 - 1.0
# Interpreting Average Silhouette Width
plot(pam1,which=2)

library(GGally)
ggpairs(clust_dat, aes(color=cluster))

```

For the PAM clustering model, all 4 of my numeric-only variables were used. From the PAM analysis, I found that the optimal k value is 3 as seen from the for loop as well as the ggplot, with an average silhouette width of 0.7160523, just about over the 0.71 mark signaling a strong structure found. Thus, selecting 3 of my data points to be representative of the cluster centers as the central observations that minimizes the sum of distances the most. From running pam1, we can see that for cluster 1, the 21st observation of the dataset is the middlemost of the cluster, while the 38th observation is for cluster 2, and the 20th observation being the middlemost for cluster 3, with the observations for each variable listed to the right. We can see clear differences between the mediods in particular for the values of the variables: incidents_00_14 and fatalities_00_14. From running the gg-plot to visualize the clusters, we can see that they were separated pretty well with distinct boundaries if we're accounting for just two of the variables. 
    
    
### Dimensionality Reduction with PCA

```{R}
# PCA code here

pca_airline <- airline_safety1 
airline_nums <- pca_airline %>% select_if(is.numeric) %>% scale
rownames(airline_nums)<-pca_airline$Name
airline_pca<-princomp(airline_nums)
names(airline_pca)

summary(airline_pca, loadings=T)

eigval <-  airline_pca$sdev^2 #square to convert SDs to eigenvalues
eigval # eigenvalues
varprop=round(eigval/sum(eigval), 2) #proportion of var explained by each PC

ggplot() + geom_bar(aes(y=varprop, x=1:4), stat="identity") + xlab("") + geom_path(aes(y=varprop, x=1:4)) + 
  geom_text(aes(x=1:4, y=varprop, label=round(varprop, 2)), vjust=1, col="white", size=5) + 
  scale_y_continuous(breaks=seq(0, .6, .2), labels = scales::percent) + 
  scale_x_continuous(breaks=1:10) # Pick PCs until cumulative proportion of variance is > 80%

round(cumsum(eigval)/sum(eigval), 2) # cumulative proportion of variance # keep the first 2

#save the first two PCs in a dataframe
airlinedf <-  data.frame(PC1=airline_pca$scores[, 1], PC2=airline_pca$scores[, 2])

#plot them!
ggplot(airlinedf, aes(PC1, PC2)) + geom_point()

## which are high/lo on PC1 and PC2? 
#highest on PC1
airline_pca$scores %>% as.data.frame %>% top_n(3, Comp.1) 

#lowest on PC1
airline_pca$scores %>% as.data.frame %>% top_n(3, wt=desc(Comp.1))

#highest on PC2
airline_pca$scores %>% as.data.frame %>% top_n(3, wt=Comp.2) 

#lowest on PC2
airline_pca$scores %>% as.data.frame %>% top_n(3, wt=desc(Comp.2)) 

## Make a nice byplot quickly:
library(factoextra)
fviz_pca_biplot(airline_pca)

```

For the PCA dimensionality reduction analysis, using the numeric variables only, we can see that the 4 variables were converted to 2 uncorrelated variables (PCs). After graphing the cumulative proportion of variances and also listing them, I chose to keep the first two components, as 0.62+0.25=0.87 eclipsed the 0.80 mark. Thus, the dataset was reduced in this process that still keeps most of the information necessary. From the plot, we can see that the direction of the axis to the left with an upward slope, is the first principal component with the greatest variation. We can then see that the perpendicular direction to the right is the second principal component with the next greatest variance. From the summary command, we can see that PC1 alone captured about 62% of all the variance across all 4 of the variables used. PC2 then captured about 25% of all the variance across the variables, so when adding these two values together, this equals 0.87. From the summary, and knowing our PCA's (1+2), we can see that PCA1 has a decently high average for all 4 variables, averaging out to about 0.5 for each of them. Thus, the high scores for PCA1 represent the airlines on average, high positive values for each variable: the highest for Incidents_00_14, third highest for the available_seat_km_per_week variable but still pretty high, and the second highest for each of the two other numeric variables. The low scores for PCA1 representing the opposite, lower values on all 4 variables. We can see that PCA2 has a high value for the avail_seat_km_per_week variable and incidents_00_14 as well. Thus, we can conclude that the high scores for PCA2 tend to have high values for these two variables and very low values for fatal_accidents_00_14 and fatalities_00_14, while the low scores tend to have the opposite values for fatal_accidents_00_14 and fatalities_00_14. It's interesting to see with the PCA analysis, that we can understand how airlines score on each component given their observations for each numeric variable used for the model.


###  Linear Classifier

```{R}
# linear classifier code here

logistic_fit <- glm(incl_reg_subsidiaries=="TRUE" ~ avail_seat_km_per_week + incidents_00_14 +
                    fatal_accidents_00_14 + fatalities_00_14, data=airline_safety1, family="binomial")

prob_reg <- predict(logistic_fit, newdata=airline_safety1, type="response")

class_diag(prob_reg, airline_safety1$incl_reg_subsidiaries, positive="TRUE", cutoff=0.5)

# cor matrix
cormat <- airline_safety1 %>% select_if(is.numeric) %>% cor(use="pair")
cormat

```

```{R}
# cross-validation of linear classifier here

set.seed(322)
k=10

data<-sample_frac(airline_safety1) #randomly order rows
folds <- rep(1:k, length.out=nrow(data)) #create folds

diags<-NULL

i=1
for(i in 1:k){
# create training and test sets
train<-data[folds!=i,] 
test<-data[folds==i,] 
truth<-test$incl_reg_subsidiaries

fit <- glm(incl_reg_subsidiaries=="TRUE" ~ avail_seat_km_per_week + incidents_00_14 +
                    fatal_accidents_00_14 + fatalities_00_14, data=train, family="binomial") ### SPECIFY THE LOGISTIC REGRESSION MODEL FIT TO THE TRAINING SET HERE

# test model
probs <- predict(fit, newdata=test, type="response") ### GET PREDICTIONS FROM THE TRAINED MODEL ON THE TEST SET HERE

# get performance metrics for each fold
diags<-rbind(diags,class_diag(probs,truth,positive="TRUE")) }

# average performance metrics across all folds
summarize_all(diags,mean)

# Cor matrix
cormat <- airline_safety1 %>% select_if(is.numeric) %>% cor(use="pair")
cormat

```

For the Linear Classifier (Logistic Regression Model), I attempted to predict the "TRUE" values for the binary variable: incl_reg_subsidiaries, which refers to if an airline included regional subsidiaries. For the predictor variables, I included the rest of the numeric variables in the dataset. Attempting to generate predicted score/probability for my original observations, the AUC score came out to be 0.8344. When performing 10-fold cross validation, I noticed a substantial decrease in the AUC score. It dropped approximately by 10%, to 0.73139, which shows signs of overfitting. It isn't a crazy drop in performance, but still substantial enough to be able to say that when predicting on the original dataset, the model fit close to quirks with its boundaries made and thus, overfit with new data. 

### Non-Parametric Classifier

```{R}
library(caret)
# non-parametric classifier code here

knn_fit <- knn3(factor(incl_reg_subsidiaries=="TRUE", 
                       levels=c("TRUE","FALSE")) ~ avail_seat_km_per_week + incidents_00_14 +
                    fatal_accidents_00_14 + fatalities_00_14, data=airline_safety1)

#your code here

prob_knn <- predict(knn_fit,airline_safety1)[,1]

# prob_knn

class_diag(prob_knn, airline_safety1$incl_reg_subsidiaries, positive="TRUE")

```

```{R}
# cross-validation of np classifier here

set.seed(322)
k=10

data<-sample_frac(airline_safety1) #randomly order rows
folds <- rep(1:k, length.out=nrow(data)) #create folds

diags<-NULL

i=1
for(i in 1:k){
# create training and test sets
train<-data[folds!=i,] 
test<-data[folds==i,] 
truth<-test$incl_reg_subsidiaries

# train model
fit <- knn3(factor(incl_reg_subsidiaries=="TRUE",levels=c("TRUE","FALSE")) ~ avail_seat_km_per_week +
              incidents_00_14 + fatal_accidents_00_14 + fatalities_00_14, data=train) ### SPECIFY THE KNN MODEL FIT TO THE TRAINING SET HERE

# test model
probs <- predict(fit, newdata=test)[,1] ### GET PREDICTIONS FROM THE TRAINED MODEL ON THE TEST SET HERE

# get performance metrics for each fold
diags<-rbind(diags,class_diag(probs,truth, positive="TRUE")) }

#average performance metrics across all folds
summarize_all(diags,mean)

```
For the Non-Parametric Classifier (K-Nearest-Neighbor), I attempted to predict the "TRUE" values for the binary variable: incl_reg_subsidiaries, again using the same predictor variables used for the Logistic Regression Model. Attempting to generate predicted score/probability for my original observations, the AUC score came out to be 0.8727, greater than the performance of the Logistic Regression Model by about 4%. However, When performing 10-fold cross validation, I noticed a pretty big drop in the AUC score. The score dropped by approximately 21%, to 0.66125, which definitely shows signs of overfitting. This is a big enough drop to be able to say that there is overfitting going on, and that the boundaries made when predicting on the original data, did not translate well over to new data. When comparing the two models, I would have to go with the linear classifier because it performed better than the non-parametric classifier by about 7% on new data, which is the performance that reflects the overall efficiency and robustness, much more than predicted scores/probabilities on original data. That being said, the KNN model did not perform *that* much better, but nonetheless a decent amount greater than the GLM model, producing a score greater than the 70% mark which is satisfactory. 


### Regression/Numeric Prediction

```{R}
# regression model code here

fit<-lm(fatalities_00_14 ~ incidents_00_14 + fatal_accidents_00_14,data=airline_safety1) 
yhat<-predict(fit) #predicted incidents_00_14

mean((airline_safety1$fatalities_00_14-yhat)^2) #mean squared error (MSE)

```

```{R}
# cross-validation of regression model here

set.seed(1234)
k=5 #choose number of folds

data<-airline_safety1[sample(nrow(airline_safety1)),] #randomly order rows
folds<-cut(seq(1:nrow(airline_safety1)),breaks=k,labels=F) #create folds

diags<-NULL
for(i in 1:k){
  train<-data[folds!=i,]
  test<-data[folds==i,]
  
  ## Fit linear regression model to training set
  fit<-lm(fatalities_00_14 ~ incidents_00_14 + fatal_accidents_00_14,data=train)
  
  ## Get predictions/y-hats on test set (fold i)
  yhat<-predict(fit,newdata=test)
  
  ## Compute prediction error  (MSE) for fold i
  diags<-mean((test$fatalities_00_14-yhat)^2) 
}

mean(diags) ## get average MSE across all folds (much higher error)!


```

Fitting the Linear Regression Model to the entire dataset, I chose to predict the numeric variable: fatalities_00_14 which assesses the number of fatalities from 2000 to 2014, from two other numeric predictor variables: incidents_00_14 and fatal_accidents_00_14. These three variables really make up the bulk of my dataset, and 3 of the 4 numeric variables, the one other being the available seat km per week amount. There was such a huge contrast between this variable to the 3 chosen numeric variables for this model, that I decided to leave it out in order to produce cleaner and more understandable results for myself. After fitting the model, the reported MSE score for the dataset was 5937.666. This shows how close the regression line is to the data points, so the distances from the points to the line or errors amounted to the square root of 5937.666, which equals about 77.056. When performing 5-fold cross-validation, the reported average MSE score was 20856.34. This definitely shows that the model overfit on the overall dataset, as the MSE score increased by a huge amount from 5937.666 to 20856.34, equaling to about 3.5 times the original score.

### Python 

```{R}
library(reticulate)
use_python("/usr/bin/python3")

```

```{python}
# python code here

#print(r.airline_safety1)
airline_py = r.airline_safety1

# Max number of fatalaties
print(r.airline_safety1.fatalities_00_14.max())

# Top 5 airlines with the most fatalities from the years 2000 - 2014
print(r.airline_safety1.filter(["airline", "fatalities_00_14"]).sort_values(by="fatalities_00_14", 
ascending=False).head(5))
 
```

```{R}

# min 10: incidents with airline name
py$airline_py %>% arrange(incidents_00_14) %>% 
  select(airline, incidents_00_14) %>% head(10)

# max 10: incidents with airline name
py$airline_py %>% arrange(desc(incidents_00_14)) %>% 
  select(airline, incidents_00_14) %>% head(10)


```

For the python code, after loading reticulate, I wanted to find some summary statistics using dplyr emulating tools. So, in the python code chunk using .r to load my dataset, I found the max number of incidents from 2000 to 2014, and the top 5 airlines with the most fatalities from 2000 to 2014. Additionally, I set the variable airline_py equal to the dataset: r.airline_safety, so I could run code on the dataset in a separate r code chunk. Subsequently, I used dpylr tools on the python dataset I loaded with py$, and ran code we learned throughout the semester to find the 10 minimum and maximum number of incidents with the airline name. The reason why I chose 10 was because the minimum value is 0, and there were 9 instances with this value.



